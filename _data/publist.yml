- title: "Asymptotically Optimal Exact Minibatch Metropolis-Hastings"
  award: Spotlight
  image: tunamh.png
  authors: Ruqi Zhang\*, A. Feder Cooper\*, Christopher De Sa
  description: Metropolis-Hastings (MH) is a commonly-used MCMC algorithm, but it can be intractable on large datasets due to requiring computations over the whole dataset. In this paper, we study <i>minibatch MH</i> methods, which instead use subsamples to enable scaling. We observe that most existing minibatch MH methods are inexact (i.e. they may change the target distribution), and show that this inexactness can cause arbitrarily large errors in inference. We propose a new exact minibatch MH method, <i>TunaMH</i>, which exposes a tunable trade-off between its minibatch size and its theoretically guaranteed convergence rate. We prove a lower bound on the batch size that any minibatch MH method <i>must</i> use to retain exactness while guaranteeing fast convergence---the first such bound for minibatch MH---and show TunaMH is asymptotically optimal in terms of the batch size. Empirically, we show TunaMH outperforms other exact minibatch MH methods on robust linear regression, truncated Gaussian mixtures, and logistic regression.
  link:
    url: https://proceedings.neurips.cc/paper/2020/hash/e2a7555f7cabd6e31aef45cb8cda4999-Abstract.html
    display:  Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020)
  highlight: 1

- title: "Random Reshuffling is Not Always Better"
  award: Spotlight
  image: shuffle.jpg
  authors: Christopher De Sa
  description: "Many learning algorithms, such as stochastic gradient descent, are affected by the order in which training examples are used. It is often observed that sampling the training examples without replacement, also known as random reshuffling, causes learning algorithms to converge faster. We give a counterexample to the Operator Inequality of Noncommutative Arithmetic and Geometric Means, a longstanding conjecture that relates to the performance of random reshuffling in learning algorithms (Recht and Ré, \"Toward a noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences,\" COLT 2012). We use this to give an example of a learning task and algorithm for which with-replacement random sampling actually outperforms random reshuffling."
  link:
    url: https://proceedings.neurips.cc/paper/2020/hash/42299f06ee419aa5d9d07798b56779e2-Abstract.html
    display:  Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020)
  highlight: 1

- title: "Neural Manifold Ordinary Differential Equations"
  authors: Aaron Lou\*, Derek Lim\*, Isay Katsman\*, Leo Huang, Qingxuan Jiang, Ser Nam Lim, Christopher M. De Sa
  description: "Many learning algorithms, such as stochastic gradient descent, are affected by the order in which training examples are used. It is often observed that sampling the training examples without replacement, also known as random reshuffling, causes learning algorithms to converge faster. We give a counterexample to the Operator Inequality of Noncommutative Arithmetic and Geometric Means, a longstanding conjecture that relates to the performance of random reshuffling in learning algorithms (Recht and Ré, \"Toward a noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences,\" COLT 2012). We use this to give an example of a learning task and algorithm for which with-replacement random sampling actually outperforms random reshuffling."
  link:
    url: https://proceedings.neurips.cc/paper/2020/hash/cbf8710b43df3f2c1553e649403426df-Abstract.html
    display:  Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020)
  highlight: 0

- title: "Moniqua: Modulo Quantized Communication in Decentralized SGD"
  authors:  Yucheng Lu, Christopher De Sa
  link:
    url: http://proceedings.mlr.press/v119/lu20a.html
    display: "Proceedings of the 37th International Conference on Machine Learning (ICML 2020)"
  highlight: 0

- title: "Differentiating through the Fréchet Mean"
  authors:  Aaron Lou\*, Isay Katsman\*, Qingxuan Jiang\*, Serge Belongie, Ser-Nam Lim, Christopher De Sa
  link:
    url: https://arxiv.org/abs/2002.11787
    display: "Proceedings of the 37th International Conference on Machine Learning (ICML 2020)"
  highlight: 0

- title: "Regulating Accuracy-Efficiency Trade-Offs in Distributed Machine Learning Systems"
  authors:  A. Feder Cooper, Karen Levy, Christopher De Sa
  award: Oral
  link:
    url: https://arxiv.org/abs/2007.02203
    display: "ICML Workshop on Law and Machine Learning (LML @ ICML 2020)"
  highlight: 0

- title: "Neural Manifold Ordinary Differential Equations"
  authors:  Aaron Lou\*, Derek Lim\*, Isay Katsman\*, Leo Huang\*, Qingxuan Jiang, Ser-Nam Lim, Christopher De Sa
  link:
    url: https://arxiv.org/abs/2006.10254
    display: "ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood (INNF+ @ ICML 2020)"
  highlight: 0

- title: "AMAGOLD: Amortized Metropolis Adjustment for Efficient Stochastic Gradient MCMC"
  authors: Ruqi Zhang, A. Feder Cooper, Christopher De Sa
  link:
    url: http://proceedings.mlr.press/v108/zhang20e.html
    display: "The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)"
  highlight: 0
